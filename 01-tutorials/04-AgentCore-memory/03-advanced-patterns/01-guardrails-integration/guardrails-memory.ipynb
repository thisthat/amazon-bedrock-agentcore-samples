{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a5400a",
   "metadata": {},
   "source": [
    "# Safeguarding conversations with Amazon Bedrock Guardrails and AgentCore Memory\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to integrate Amazon Bedrock Guardrails with AgentCore Memory to create a secure conversational agent. You'll build an agent that filters sensitive content while maintaining conversation context across interactions.\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                          |\n",
    "|:--------------------|:-----------------------------------------------------------------|\n",
    "| Tutorial type       | Guardrails / Memory Integration                                  |\n",
    "| Agent type          | Safeguarded Memory-Enabled Agent                                 |\n",
    "| Agentic Framework   | Strands Agents                                                   |\n",
    "| LLM model           | Anthropic Claude Sonnet 3.7                                      |\n",
    "| Key features        | Guardrails, Memory Integration, Content Filtering                |\n",
    "| Example complexity  | Intermediate                                                     |\n",
    "| SDK used            | Amazon Bedrock Python SDK and Bedrock Memory SDK                 |\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this tutorial, you'll learn:\n",
    "1. How to create a memory resource for your agent\n",
    "2. How to implement Amazon Bedrock Guardrails with content filtering\n",
    "3. How to build a custom hook that combines guardrails and memory functionality\n",
    "4. How to selectively store safe conversation history\n",
    "5. How to test your secure agent with different types of content\n",
    "\n",
    "### Architecture\n",
    "\n",
    "This example demonstrates the integration of guardrails with memory for secure conversations:\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"guardrails_memory_flow.png\" width=\"90%\"/>\n",
    "</div>\n",
    "\n",
    "## 0. Prerequisites\n",
    "\n",
    "To execute this tutorial you will need:\n",
    "* Python 3.10+\n",
    "* AWS credentials configured with access to AgentCore Memory and Amazon Bedrock\n",
    "* Amazon Bedrock model access (Claude 3.7 Sonnet)\n",
    "* Amazon Bedrock Memory SDK\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1217ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import boto3\n",
    "import uuid\n",
    "import logging\n",
    "from typing import Dict\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "from botocore.exceptions import ClientError\n",
    "from strands.hooks import AgentInitializedEvent, HookProvider, HookRegistry, MessageAddedEvent\n",
    "from strands.experimental.hooks import AfterModelInvocationEvent\n",
    "\n",
    "# Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"secure-agent\")\n",
    "REGION = os.getenv('AWS_REGION', 'us-west-2') # AWS region for the agent\n",
    "bedrock_client = boto3.client('bedrock', region_name=REGION)\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=REGION)\n",
    "memory_client = MemoryClient(region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9b22e",
   "metadata": {},
   "source": [
    "## 1. Creating Amazon Bedrock Guardrails\n",
    "\n",
    "In this section, we'll create a guardrail to enforce content safety policies for our agent. Guardrails act as safety filters that can be applied to both user inputs and model outputs. For our example, we'll create a guardrail with two specific policies:\n",
    "\n",
    "1. **Input Filtering**: Block insulting language from users\n",
    "2. **Output Filtering**: Prevent the model from discussing political topics\n",
    "\n",
    "This approach demonstrates how guardrails can protect against different types of problematic content from both directions of the conversation. The input filter helps maintain a respectful conversation environment, while the output filter ensures the model doesn't discuss potentially sensitive topics.\n",
    "\n",
    "The end goal is to prevent saving unwanted messages in our memory, ensuring that only appropriate content is stored for future context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique identifier for this request\n",
    "unique_id = str(uuid.uuid4())[:6]\n",
    "\n",
    "# Define guardrail configuration\n",
    "guardrail_name = f\"SecureConversationGuardrail_{unique_id}\"\n",
    "guardrail_description = \"Blocks insults in input and political content in output\"\n",
    "\n",
    "try:\n",
    "    # Create the guardrail\n",
    "    response = bedrock_client.create_guardrail(\n",
    "        name=guardrail_name,\n",
    "        description=guardrail_description,\n",
    "        # Block insults in input\n",
    "        contentPolicyConfig={\n",
    "            'filtersConfig': [\n",
    "                {\n",
    "                    'type': 'INSULTS',\n",
    "                    'inputStrength': 'MEDIUM',\n",
    "                    'outputStrength': 'MEDIUM',\n",
    "                    'inputModalities': ['TEXT'],\n",
    "                    'outputModalities': ['TEXT'],\n",
    "                    'inputAction': 'BLOCK',\n",
    "                    'outputAction': 'NONE',\n",
    "                    'inputEnabled': True,\n",
    "                    'outputEnabled': False\n",
    "                }\n",
    "            ],\n",
    "            'tierConfig': {\n",
    "                'tierName': 'CLASSIC'\n",
    "            }\n",
    "        },\n",
    "        # Block political content in output\n",
    "        topicPolicyConfig={\n",
    "            'topicsConfig': [\n",
    "                {\n",
    "                    'name': 'Politics',\n",
    "                    'definition': 'Content related to political leaders, elections, political parties, or government affairs',\n",
    "                    'examples': [\n",
    "                        'Who is the current president?',\n",
    "                        'Tell me about the upcoming election',\n",
    "                        'Explain the political situation in Congress'\n",
    "                    ],\n",
    "                    'type': 'DENY',\n",
    "                    'inputAction': 'NONE',\n",
    "                    'outputAction': 'BLOCK',\n",
    "                    'inputEnabled': False,\n",
    "                    'outputEnabled': True\n",
    "                }\n",
    "            ],\n",
    "            'tierConfig': {\n",
    "                'tierName': 'CLASSIC'\n",
    "            }\n",
    "        },\n",
    "        blockedInputMessaging=\"I'm sorry, but your message contains inappropriate language. Please rephrase your question without insults.\",\n",
    "        blockedOutputsMessaging=\"I apologize, but I cannot provide information on political topics. Is there something else I can help you with?\",\n",
    "    )\n",
    "    \n",
    "    # Store guardrail ID for later use\n",
    "    guardrail_id = response['guardrailId']\n",
    "    guardrail_arn = response['guardrailArn']\n",
    "    guardrail_version = \"DRAFT\"  # New guardrails are created as DRAFT\n",
    "    \n",
    "    print(f\"✅ Created guardrail: {guardrail_id} (ARN: {guardrail_arn})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating guardrail: {e}\")\n",
    "    # If the guardrail already exists, try to find its ID\n",
    "    try:\n",
    "        response = bedrock_client.list_guardrails()\n",
    "        existing_guardrail = next((g for g in response['guardrailSummaries'] \n",
    "                                  if g['name'] == guardrail_name), None)\n",
    "        if existing_guardrail:\n",
    "            guardrail_id = existing_guardrail['guardrailId']\n",
    "            guardrail_version = \"DRAFT\"  # Use DRAFT version\n",
    "            print(f\"Using existing guardrail: {guardrail_id}\")\n",
    "    except Exception as list_error:\n",
    "        print(f\"❌ Error listing guardrails: {list_error}\")\n",
    "        guardrail_id = None\n",
    "        guardrail_version = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc31ebd",
   "metadata": {},
   "source": [
    "## 2. Creating Memory Resource\n",
    "\n",
    "In this section, we'll create a memory resource for our agent to store conversation history. Memory allows the agent to recall past interactions, maintain context, and provide more coherent responses over time. By combining memory with guardrails, we can ensure that only appropriate content is stored for future reference.\n",
    "\n",
    "For this example, we'll create a simple short-term memory resource without any additional strategies, which is perfect for maintaining conversation context within a session. The memory will store messages that have passed our guardrail checks, ensuring that inappropriate content is filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1bcfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_name = f\"SecureAgentMemory_{unique_id}\"\n",
    "\n",
    "try:\n",
    "    # Create memory resource without strategies (thus only access to short-term memory)\n",
    "    memory = memory_client.create_memory_and_wait(\n",
    "        name=memory_name,\n",
    "        strategies=[],  # No strategies for short-term memory\n",
    "        description=\"Short-term memory for personal agent with guardrails\",\n",
    "        event_expiry_days=7,\n",
    "    )\n",
    "    memory_id = memory['id']\n",
    "    logger.info(f\"✅ Created memory: {memory_id}\")\n",
    "except ClientError as e:\n",
    "    logger.info(f\"❌ ERROR: {e}\")\n",
    "    if e.response['Error']['Code'] == 'ValidationException' and \"already exists\" in str(e):\n",
    "        # If memory already exists, retrieve its ID\n",
    "        memories = memory_client.list_memories()\n",
    "        memory_id = next((m['id'] for m in memories if m['id'].startswith(memory_name)), None)\n",
    "        logger.info(f\"Memory already exists. Using existing memory ID: {memory_id}\")\n",
    "except Exception as e:\n",
    "    # Show any errors during memory creation\n",
    "    logger.error(f\"❌ ERROR: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    # Cleanup on error - delete the memory if it was partially created\n",
    "    if 'memory_id' in locals() and memory_id:\n",
    "        try:\n",
    "            memory_client.delete_memory_and_wait(memory_id=memory_id)\n",
    "            logger.info(f\"Cleaned up memory: {memory_id}\")\n",
    "        except Exception as cleanup_error:\n",
    "            logger.error(f\"Failed to clean up memory: {cleanup_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712a8d6",
   "metadata": {},
   "source": [
    "## 3. Integrating Bedrock Guardrails, Strands and Agentcore Memory\n",
    "\n",
    "In this section, we'll create custom hooks that integrate guardrails with memory functionality. Our implementation will:\n",
    "\n",
    "1. Check both user inputs and model outputs using Amazon Bedrock Guardrails\n",
    "2. Replace inappropriate content with safe alternatives\n",
    "3. Only store messages in memory that have passed our guardrail checks\n",
    "4. Retrieve past conversation context from memory when the agent is initialized\n",
    "\n",
    "This approach ensures that our agent maintains a clean conversation history while still benefiting from memory capabilities. Let's build the necessary components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuardrailsEvaluator:\n",
    "    \"\"\"Reusable guardrails evaluation utility.\"\"\"\n",
    "    \n",
    "    def __init__(self, guardrail_id: str, guardrail_version: str):\n",
    "        \"\"\"Initialize the guardrails evaluator.\n",
    "        \n",
    "        Args:\n",
    "            guardrail_id: The ID of the guardrail to use\n",
    "            guardrail_version: The version of the guardrail (e.g., \"DRAFT\")\n",
    "        \"\"\"\n",
    "        self.guardrail_id = guardrail_id\n",
    "        self.guardrail_version = guardrail_version\n",
    "    \n",
    "    def evaluate_content(self, content: str, source: str) -> Dict:\n",
    "        \"\"\"Evaluate content using Bedrock Guardrails and return result.\n",
    "        \n",
    "        Args:\n",
    "            content: The text content to evaluate\n",
    "            source: The source type (\"INPUT\" or \"OUTPUT\")\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing guardrail evaluation results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.warning(f\"⏳ CHECKING {source}: '{content[:30]}...'\")\n",
    "            \n",
    "            response = bedrock_runtime_client.apply_guardrail(\n",
    "                guardrailIdentifier=self.guardrail_id,\n",
    "                guardrailVersion=self.guardrail_version,\n",
    "                source=source,\n",
    "                content=[{\"text\": {\"text\": content}}]\n",
    "            )\n",
    "            \n",
    "            action = response.get('action')\n",
    "            logger.warning(f\"🔍 GUARDRAIL ACTION: {action}\")\n",
    "            \n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Guardrail evaluation failed: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "class GuardrailsMemoryHookProvider(HookProvider):\n",
    "    \"\"\"Hook provider that combines guardrails enforcement with memory storage.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        memory_client: MemoryClient, \n",
    "        memory_id: str, \n",
    "        actor_id: str, \n",
    "        session_id: str,\n",
    "        guardrails_evaluator: GuardrailsEvaluator\n",
    "    ):\n",
    "        \"\"\"Initialize the hook provider.\n",
    "        \n",
    "        Args:\n",
    "            memory_client: Client for memory storage\n",
    "            memory_id: ID of the memory to use\n",
    "            actor_id: ID of the actor\n",
    "            session_id: ID of the current session\n",
    "            guardrails_evaluator: Evaluator for content validation\n",
    "        \"\"\"\n",
    "        # Memory properties\n",
    "        self.memory_id = memory_id\n",
    "        self.actor_id = actor_id\n",
    "        self.session_id = session_id\n",
    "        \n",
    "        # Guardrails evaluator\n",
    "        self.evaluator = guardrails_evaluator\n",
    "        \n",
    "        # Message tracking system\n",
    "        self.blocked_outputs = set()  # Model outputs that failed guardrail checks\n",
    "\n",
    "    def after_model_invocation(self, event: AfterModelInvocationEvent) -> None:\n",
    "        \"\"\"Check model output with guardrails and replace if needed.\n",
    "        \n",
    "        Args:\n",
    "            event: Event containing the model response\n",
    "        \"\"\"\n",
    "        # Skip if model invocation failed\n",
    "        if event.exception is not None or event.stop_response is None:\n",
    "            logger.error(\"⚠️ Model invocation failed, skipping guardrail check\")\n",
    "            return\n",
    "        \n",
    "        logger.info(\"🔍 AfterModelInvocationEvent: Checking model output\")\n",
    "        \n",
    "        # Extract message from the model response\n",
    "        message = event.stop_response.message\n",
    "        \n",
    "        # Extract content\n",
    "        if isinstance(message.get(\"content\"), list):\n",
    "            content = \"\".join(block.get(\"text\", \"\") for block in message.get(\"content\", []))\n",
    "        else:\n",
    "            content = str(message.get(\"content\", \"\"))\n",
    "        \n",
    "        content_id = hash(content)\n",
    "        \n",
    "        # Check against guardrails\n",
    "        result = self.evaluator.evaluate_content(content, \"OUTPUT\")\n",
    "        \n",
    "        # Handle guardrail violations\n",
    "        if result.get(\"action\") == \"GUARDRAIL_INTERVENED\":\n",
    "            logger.warning(\"⛔ ASSISTANT MESSAGE BLOCKED BY GUARDRAILS\")\n",
    "            \n",
    "            # Mark this output as blocked\n",
    "            self.blocked_outputs.add(content_id)\n",
    "            \n",
    "            # Get the guardrail-provided alternative if available\n",
    "            replacement_content = None\n",
    "            if \"outputs\" in result and result[\"outputs\"] and len(result[\"outputs\"]) > 0:\n",
    "                if \"text\" in result[\"outputs\"][0]:\n",
    "                    replacement_content = result[\"outputs\"][0][\"text\"]\n",
    "            \n",
    "            # Fall back to generic message if no replacement provided\n",
    "            if not replacement_content:\n",
    "                replacement_content = \"I apologize, but I cannot provide the requested information as it would violate our content policies.\"\n",
    "            \n",
    "            # Update the message content - THIS WILL CHANGE WHAT THE USER SEES\n",
    "            if isinstance(message.get(\"content\"), list):\n",
    "                message[\"content\"] = [{\"text\": replacement_content}]\n",
    "            else:\n",
    "                message[\"content\"] = replacement_content\n",
    "            \n",
    "            logger.info(f\"⚠️ Replaced assistant message with guardrail response: {replacement_content[:30]}...\")\n",
    "    \n",
    "    def on_message_added(self, event: MessageAddedEvent) -> None:\n",
    "        \"\"\"Store messages in memory if they pass checks.\n",
    "        \n",
    "        Args:\n",
    "            event: Event containing the message to store\n",
    "        \"\"\"\n",
    "        # Get the message\n",
    "        message = event.message\n",
    "        \n",
    "        # Skip if no message\n",
    "        if not message:\n",
    "            return\n",
    "        \n",
    "        # Extract content\n",
    "        if isinstance(message.get(\"content\"), list):\n",
    "            content = \"\".join(block.get(\"text\", \"\") for block in message.get(\"content\", []))\n",
    "        else:\n",
    "            content = str(message.get(\"content\", \"\"))\n",
    "        \n",
    "        role = message.get(\"role\", \"\")\n",
    "        content_id = hash(content)\n",
    "        \n",
    "        # Skip system messages\n",
    "        if role == \"system\":\n",
    "            return\n",
    "        \n",
    "        # Skip if this specific message was blocked (only applies to assistant messages)\n",
    "        if role == \"assistant\" and content_id in self.blocked_outputs:\n",
    "            logger.info(f\"⛔ Skipping memory storage for blocked {role} message\")\n",
    "            return\n",
    "        \n",
    "        # Store the message in memory\n",
    "        try:\n",
    "            logger.info(f\"💾 Storing {role} message in memory (passed guardrail check)\")\n",
    "            memory_client.create_event(\n",
    "                memory_id=self.memory_id,\n",
    "                actor_id=self.actor_id,\n",
    "                session_id=self.session_id,\n",
    "                messages=[(content, role)]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Memory save error: {e}\")\n",
    "    \n",
    "    def on_agent_initialized(self, event: AgentInitializedEvent) -> None:\n",
    "        \"\"\"Load recent conversation history when agent starts.\n",
    "        \n",
    "        Args:\n",
    "            event: Event containing the agent that was initialized\n",
    "        \"\"\"\n",
    "        try:\n",
    "            recent_turns = memory_client.get_last_k_turns(\n",
    "                memory_id=self.memory_id,\n",
    "                actor_id=self.actor_id,\n",
    "                session_id=self.session_id,\n",
    "                k=5\n",
    "            )\n",
    "            \n",
    "            if recent_turns:\n",
    "                context_messages = []\n",
    "                for turn in recent_turns:\n",
    "                    for message in turn:\n",
    "                        role = message['role']\n",
    "                        content = message['content']['text']\n",
    "                        context_messages.append(f\"{role}: {content}\")\n",
    "                \n",
    "                context = \"\\n\".join(context_messages)\n",
    "                event.agent.system_prompt += f\"\\n\\nRecent conversation:\\n{context}\"\n",
    "                logger.info(f\"✅ Loaded {len(recent_turns)} conversation turns\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Memory load error: {e}\")\n",
    "\n",
    "    def register_hooks(self, registry: HookRegistry):\n",
    "        \"\"\"Register all hooks with the registry.\n",
    "        \n",
    "        Args:\n",
    "            registry: The hook registry to register with\n",
    "        \"\"\"\n",
    "        registry.add_callback(AfterModelInvocationEvent, self.after_model_invocation)\n",
    "        registry.add_callback(MessageAddedEvent, self.on_message_added)\n",
    "        registry.add_callback(AgentInitializedEvent, self.on_agent_initialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a7892",
   "metadata": {},
   "source": [
    "## 4. Creating and configuring the Agent\n",
    "\n",
    "In this section, we'll create our secure conversational agent by combining all the components we've built: the Bedrock model, guardrails evaluator, and memory-enabled hook provider. This integration creates a complete agent that can maintain conversations while enforcing content policies and storing appropriate context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90350394",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\"\n",
    "bedrock_model = BedrockModel(model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "\n",
    "evaluator = GuardrailsEvaluator(\n",
    "    guardrail_id=guardrail_id,\n",
    "    guardrail_version=guardrail_version\n",
    ")\n",
    "\n",
    "def create_personal_agent():\n",
    "    \"\"\"Create personal agent with memory and guardrails\"\"\"\n",
    "    agent = Agent(\n",
    "        name=\"PersonalAssistant\",\n",
    "        model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        system_prompt=\"You are a helpful personal assistant. Be friendly and professional.\",\n",
    "        hooks=[GuardrailsMemoryHookProvider(memory_client, memory_id, ACTOR_ID, SESSION_ID, evaluator)],\n",
    "        callback_handler=None\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "# Create agent\n",
    "agent = create_personal_agent()\n",
    "logger.info(\"✅ Personal agent created with memory and guardrails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631317d",
   "metadata": {},
   "source": [
    "This implementation creates a secure agent that will:\n",
    "\n",
    "1. Load existing conversation context from memory when initialized\n",
    "2. Check user inputs against guardrails before processing\n",
    "3. Check model outputs against guardrails before showing to the user\n",
    "4. Store only approved messages in memory for future context\n",
    "5. Maintain conversation history across multiple interactions\n",
    "\n",
    "The combination of guardrails and memory ensures that our agent maintains a secure but contextual conversation experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb04a9",
   "metadata": {},
   "source": [
    "## 5. Testing the Secure Agent\n",
    "\n",
    "Let's test our agent with different types of input to see how the guardrails and memory integration work in practice. We'll try both acceptable inputs and those that might trigger guardrail interventions to verify our implementation is working correctly.\n",
    "\n",
    "First, let's create a helper function to handle the guardrail check and agent invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e048db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_guardrails(user_input):\n",
    "    \"\"\"Process user input with guardrails before sending to agent.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The text input from the user\n",
    "        \n",
    "    Returns:\n",
    "        The agent response or guardrail rejection\n",
    "    \"\"\"\n",
    "    # Check input against guardrails\n",
    "    result = evaluator.evaluate_content(user_input, \"INPUT\")\n",
    "    \n",
    "    if result.get(\"action\") == \"GUARDRAIL_INTERVENED\":\n",
    "        # Get rejection message from guardrail\n",
    "        if \"outputs\" in result and result[\"outputs\"] and \"text\" in result[\"outputs\"][0]:\n",
    "            rejection_content = result[\"outputs\"][0][\"text\"]\n",
    "        else:\n",
    "            rejection_content = \"I cannot process that request.\"\n",
    "        \n",
    "        # Return rejection without calling agent\n",
    "        print(rejection_content)\n",
    "        return rejection_content\n",
    "    else:\n",
    "        # Input passed guardrails, proceed with agent call\n",
    "        response = agent(user_input)\n",
    "        print(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3128d983",
   "metadata": {},
   "source": [
    "### Test 1: Normal Conversation\n",
    "\n",
    "Let's start with a normal greeting that should pass all guardrails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 1: Normal greeting\")\n",
    "user_input = \"I am dani.\"\n",
    "process_with_guardrails(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bb4e6",
   "metadata": {},
   "source": [
    "### Test 2: Insulting Content (Should Trigger Input Guardrail)\n",
    "\n",
    "Let's try input with insulting language that should be blocked by the input guardrail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest 2: Insulting content (should trigger input guardrail)\")\n",
    "user_input = \"You're a stupid assistant.\"\n",
    "process_with_guardrails(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a58c4",
   "metadata": {},
   "source": [
    "### Test 3: Political Content (Should Trigger Output Guardrail)\n",
    "\n",
    "Now let's try a question about politics, which should pass the input guardrail but trigger the output guardrail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest 3: Political question (should trigger output guardrail)\")\n",
    "user_input = \"Who is the president of the US?\"\n",
    "process_with_guardrails(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d03633",
   "metadata": {},
   "source": [
    "### Examining Memory Contents\n",
    "\n",
    "Let's check what was stored in memory after our tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's stored in memory\n",
    "print(\"\\n=== Memory Contents ===\")\n",
    "recent_turns = memory_client.get_last_k_turns(\n",
    "    memory_id=memory_id,\n",
    "    actor_id=ACTOR_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    k=5  # Adjust k to see more or fewer turns\n",
    ")\n",
    "\n",
    "for i, turn in enumerate(recent_turns):\n",
    "    print(f\"\\nTurn {i+1}:\")\n",
    "    for msg in turn:\n",
    "        role = msg['role']\n",
    "        content = msg['content']['text']\n",
    "        print(f\"- {role}: {content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fe0c7",
   "metadata": {},
   "source": [
    "### Test 4: Follow-Up Question to Test Memory\n",
    "Let's ask a follow-up question to see if the agent remembers previous context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037da2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_personal_agent()\n",
    "print(\"\\nTest 4: Follow-up to test memory\")\n",
    "user_input = \"What's my name?\"\n",
    "process_with_guardrails(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c7a33",
   "metadata": {},
   "source": [
    "## 6. Cleanup (Optional)\n",
    "\n",
    "When you're done experimenting with your secure agent, you may want to clean up the resources created in this tutorial. This section shows you how to delete the guardrail and memory resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the memory resource\n",
    "try:\n",
    "    memory_client.delete_memory_and_wait(memory_id=memory_id)\n",
    "    print(f\"✅ Deleted memory resource: {memory_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting memory: {e}\")\n",
    "\n",
    "# Delete the guardrail\n",
    "try:\n",
    "    bedrock_client.delete_guardrail(\n",
    "        guardrailIdentifier=guardrail_id\n",
    "    )\n",
    "    print(f\"✅ Deleted guardrail: {guardrail_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting guardrail: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84626a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've built a secure conversational agent that combines Amazon Bedrock Guardrails with AgentCore Memory capabilities. Our implementation:\n",
    "\n",
    "1. Filters inappropriate user inputs using guardrails\n",
    "2. Prevents the agent from discussing sensitive topics\n",
    "3. Only stores approved messages in memory\n",
    "4. Maintains conversation context using memory for enhanced user experience\n",
    "\n",
    "By integrating guardrails with memory, you can build robust agents that maintain compliance with content policies while still providing personalized and contextual responses. This pattern can be extended to more complex scenarios by adding additional guardrail filters or implementing more long term memory strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
